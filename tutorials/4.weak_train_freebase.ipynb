{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train A Retriever on WebQSP with Weak Supervision\n",
    "\n",
    "**Data**\n",
    "- Knowledge Graph: Freebase\n",
    "- Dataset: WebQSP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Install & import dependencies\n",
    "\n",
    "```bash\n",
    "pip install srtk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import srsly\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data_dir = 'data/webqsp/formatted'\n",
    "intermediate_dir = 'data/webqsp/intermediate'\n",
    "dataset_dir = 'data/webqsp/dataset'\n",
    "save_model_dir = 'artifacts/models/webqsp'\n",
    "retrieve_subgraph_path = 'artifacts/subgraphs/webqsp.jsonl'\n",
    "\n",
    "formatted_train_path = os.path.join(formatted_data_dir, 'train.jsonl')\n",
    "formatted_test_path = os.path.join(formatted_data_dir, 'test.jsonl')\n",
    "train_dataset_path = os.path.join(dataset_dir, 'train.jsonl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run SPARQL endpoint\n",
    "\n",
    "We assume the freebase SPARQL endpoint is running at `http://localhost:3001/sparql`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Download data\n",
    "\n",
    "You may download WebQSP dataset from [Microsof download center](https://www.microsoft.com/en-us/download/details.aspx?id=52763).\n",
    "\n",
    "```bash\n",
    "mkdir -p data/webqsp/raw\n",
    "wget https://download.microsoft.com/download/F/5/0/F5012144-A4FB-4084-897F-CFDA99C60BDF/WebQSP.zip -P data/webqsp\n",
    "unzip data/webqsp/WebQSP.zip -d data/webqsp/raw\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Format the raw data\n",
    "\n",
    "The raw data should be formatted like this:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"sample-id\",\n",
    "  \"question\": \"Which universities did Barack Obama graduate from?\",\n",
    "  \"question_entities\": [  ],\n",
    "  \"answer_entities\": [  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = {\n",
    "    'train': 'data/webqsp/raw/WebQSP/data/WebQSP.train.json',\n",
    "    'test': 'data/webqsp/raw/WebQSP/data/WebQSP.test.json'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3098/3098 [00:00<00:00, 37617.62it/s]\n",
      "Processing test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1639/1639 [00:00<00:00, 124835.92it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_samples = {}\n",
    "for split, split_path in raw_data_path.items():\n",
    "    raw_samples = srsly.read_json(split_path)['Questions']\n",
    "    processed_split_samples = []\n",
    "    for raw_sample in tqdm(raw_samples, desc=f'Processing {split} split'):\n",
    "        answers = set()\n",
    "        for parse in raw_sample['Parses']:\n",
    "            for answer in parse['Answers']:\n",
    "                if answer['AnswerType'] == 'Entity':\n",
    "                    answers.add(answer['AnswerArgument'])\n",
    "        sample = {\n",
    "            'question': raw_sample['ProcessedQuestion'],\n",
    "            'question_entities': list(set(e['TopicEntityMid'] for e in raw_sample['Parses'])),\n",
    "            'answer_entities': list(answers)\n",
    "        }\n",
    "        if len(sample['answer_entities']) > 0 and len(sample['question_entities']) > 0:\n",
    "            processed_split_samples.append(sample)\n",
    "    processed_samples[split] = processed_split_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(formatted_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "for split, split_samples in processed_samples.items():\n",
    "    save_path = os.path.join(formatted_data_dir, f'{split}.jsonl')\n",
    "    srsly.write_jsonl(save_path, split_samples)\n",
    "    print(f'Formatted {split} samples are saved to {save_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess and create the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2990/2990 [09:22<00:00,  5.32it/s]\n",
      "Number of training records: 9802\n",
      "Converting relation ids to labels: 100%|â–ˆ| 9802/9802 [00:00<00:00, 148940.77it/s\n",
      "Training samples are saved to data/webqsp/dataset/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "!srtk preprocess --input data/webqsp/intermediate/scores.jsonl \\\n",
    "    --output $dataset_train_path \\\n",
    "    --intermediate-dir $intermediate_dir \\\n",
    "    --sparql-endpoint http://localhost:3001/sparql \\\n",
    "    --knowledge-graph freebase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Inspect the training data that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"query\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"what country is the grand bahama island in [SEP] \"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"positive\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"location.location.containedby\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"negatives\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[0;32m\"location.location.nearby_airports\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.contains\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.time_zones\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.nearby_airports\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"kg.object_profile.prominent_type\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.statistical_region.population\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"common.topic.webpage\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.contains\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.time_zones\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"common.topic.notable_types\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.time_zones\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"common.topic.article\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.nearby_airports\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"location.location.nearby_airports\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"common.topic.notable_for\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 $dataset_train_path | jq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Found cached dataset json (/home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e33bfebaa2163bee.arrow\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-87e3c5d3b3d27ab9.arrow\n",
      "Found cached dataset json (/home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f4635320b6ca74ec.arrow\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-d39e2cbcbb9827f5/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fd833ba3bd21ffbe.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanchun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1martifacts/wandb/run-20230430_004609-2xbpdaj0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m04300046\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/2xbpdaj0\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | RobertaModel | 124 M \n",
      "---------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.583   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [01:39<00:00,  1.47it/s, v_num=daj0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [01:49<00:00,  1.34it/s, v_num=daj0]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [01:49<00:00,  1.34it/s, v_num=daj0]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [01:48<00:00,  1.34it/s, v_num=daj0]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146/146 [01:48<00:00,  1.34it/s, v_num=daj0]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "\u001b[A`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–†â–†â–†â–†â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss â–ˆâ–†â–„â–‚â–‚â–‚â–ƒâ–„â–‚â–‚â–‚â–â–â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss â–ˆâ–„â–‚â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.24391\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 729\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.19752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33m04300046\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/2xbpdaj0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1martifacts/wandb/run-20230430_004609-2xbpdaj0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 srtk train --train-dataset $train_dataset_path \\\n",
    "    --model-name-or-path roberta-base \\\n",
    "    --output-dir $save_model_dir \\\n",
    "    --accelerator gpu \\\n",
    "    --learning-rate 1e-5 \\\n",
    "    --batch-size 64 \\\n",
    "    --max-epochs 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate the Retriever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation of the retriever, simply pass `--evaluate` flag to the `retrieve` subcommand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_test_path = os.path.join(formatted_data_dir, 'test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subgraphs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1582/1582 [58:41<00:00,  2.23s/it]\n",
      "Retrieved subgraphs saved to to artifacts/subgraphs/webqsp.jsonl\n",
      "Answer recall: 0.9121365360303414 (1443 / 1582)"
     ]
    }
   ],
   "source": [
    "!srtk retrieve --input $formatted_test_path \\\n",
    "    --output $retrieve_subgraph_path \\\n",
    "    --sparql-endpoint http://localhost:3001/sparql \\\n",
    "    --knowledge-graph freebase \\\n",
    "    --scorer-model-path $save_model_dir \\\n",
    "    --beam-width 10 \\\n",
    "    --max-depth 2 \\\n",
    "    --evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
