{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Training with Wikidata Simple Questions\n",
    "\n",
    "- Dataset Repository: [askplatypus/wikidata-simplequestions](https://github.com/askplatypus/wikidata-simplequestions)\n",
    "- Stats (only answerable questions):\n",
    "    - train: 14894\n",
    "    - valid: 4295\n",
    "    - test: 2210"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import dependencies:\n",
    "\n",
    "```bash\n",
    "pip install srsly srtk pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import srsly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from GitHub to `data/wikidata-simplequestions/raw` with the following commands:\n",
    "\n",
    "```bash\n",
    "mkdir -p data/wikidata-simplequestions/raw\n",
    "wget https://raw.githubusercontent.com/askplatypus/wikidata-simplequestions/master/annotated_wd_data_test_answerable.txt -P data/wikidata-simplequestions/raw\n",
    "wget https://raw.githubusercontent.com/askplatypus/wikidata-simplequestions/master/annotated_wd_data_train_answerable.txt -P data/wikidata-simplequestions/raw\n",
    "wget https://raw.githubusercontent.com/askplatypus/wikidata-simplequestions/master/annotated_wd_data_valid_answerable.txt -P data/wikidata-simplequestions/raw\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Format the Raw Data for Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Inspect raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_paths = {\n",
    "    'train': 'data/wikidata-simplequestions/raw/annotated_wd_data_train_answerable.txt',\n",
    "    'valid': 'data/wikidata-simplequestions/raw/annotated_wd_data_valid_answerable.txt',\n",
    "    'test': 'data/wikidata-simplequestions/raw/annotated_wd_data_test_answerable.txt',\n",
    "}\n",
    "splits = ['train', 'valid', 'test']\n",
    "intermediate_dir = 'data/wikidata-simplequestions/intermediate' # intermediate data, here it's the scored paths\n",
    "dataset_dir = 'data/wikidata-simplequestions/dataset'  # preprocessed data\n",
    "output_model_dir = 'artifacts/models/wd_simple'\n",
    "retrieved_subgraph_path = 'artifacts/subgraphs/wd_simple.jsonl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Rxxx` property identifiers encode the inverse property of the Wikidata property `Pxxx`. For example `R19` encodes the properties \"born here\", i.e. the inverse of `P19` (\"birth place\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q12439\tR19\tQ6106580\twho is a musician born in detroit\n",
      "Q6817891\tP364\tQ1568\twhat is the language in which mera shikar was filmed in\n",
      "Q1297\tR276\tQ2888523\tWhats the name of a battle that happened in chicago\n",
      "Q193592\tR413\tQ5822614\twhat player plays the position midfielder?\n",
      "Q6849115\tP413\tQ336286\twhat is the position that  mike twellman plays\n"
     ]
    }
   ],
   "source": [
    "raw_train_path = raw_paths['train']\n",
    "! head -n 5 $raw_train_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Remove reverse relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train size: 19481\n",
      "train size after removing reverse relation: 14894\n",
      "76.45% percentage train data are kept\n",
      "Full train size: 2821\n",
      "valid size after removing reverse relation: 2210\n",
      "78.34% percentage valid data are kept\n",
      "Full train size: 5622\n",
      "test size after removing reverse relation: 4296\n",
      "76.41% percentage test data are kept\n"
     ]
    }
   ],
   "source": [
    "preserved_data = {}\n",
    "for split in splits:\n",
    "    raw_path = raw_paths[split]\n",
    "    data = pd.read_csv(raw_path, sep='\\t', header=None)\n",
    "    print('Full train size:', len(data))\n",
    "    before_len = len(data)\n",
    "    # Remove samples where the 1-st column starting with 'R'\n",
    "    data = data[~data[1].str.startswith('R')]\n",
    "    after_len = len(data)\n",
    "    print(f'{split} size after removing reverse relation: {len(data)}')\n",
    "    print(f'{after_len/before_len*100:.2f}% percentage {split} data are kept')\n",
    "    preserved_data[split] = data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Convert to scored path format\n",
    "\n",
    "The paths format is a JSONL file, where each line is a dictionary as:\n",
    "```json\n",
    "{\n",
    "    \"id\": \"train-100\",\n",
    "    \"question\": \"What is the birth place of Barack Obama?\",\n",
    "    \"question_entities\": [\"Q76\"],\n",
    "    \"answer_entities\": [\"Q23513\"],\n",
    "    \"paths\": [[\"P19\"]]  # there may be multiple paths, and each path may have variable lengths\n",
    "    \"scores\": [1.0]     # the score of each path. for ground truth paths, we assign max score 1.0 to each path.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store intermediate files\n",
    "!mkdir -p $intermediate_dir\n",
    "!mkdir -p $dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train scored paths file to data/wikidata-simplequestions/intermediate/scores_train.jsonl\n",
      "Saved valid scored paths file to data/wikidata-simplequestions/intermediate/scores_valid.jsonl\n",
      "Saved test scored paths file to data/wikidata-simplequestions/intermediate/scores_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    data = preserved_data[split]\n",
    "    samples = []\n",
    "    for idx, line in enumerate(data.values):\n",
    "        question_entity, relation, answer_entity, question = line\n",
    "        sample = {\n",
    "            \"id\": f\"{split}-{idx:05d}\",\n",
    "            \"question\": question,\n",
    "            \"question_entities\": [question_entity],\n",
    "            \"answer_entities\": [answer_entity],\n",
    "            \"paths\": [[relation]],\n",
    "            \"path_scores\": [1.0]\n",
    "        }\n",
    "        samples.append(sample)\n",
    "    save_path = os.path.join(intermediate_dir, f'scores_{split}.jsonl')\n",
    "    srsly.write_jsonl(save_path, samples)\n",
    "    print(f'Saved {split} scored paths file to {save_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocessing\n",
    "\n",
    "Use the `srtk preprocess` command to creating training samples. Do not pass `--search-path` beacuse the paths are already provided in the dataset. This step mainly involves negative sampling and dataset generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "Negative sampling: 100%|██████████████████| 14894/14894 [02:49<00:00, 87.90it/s]\n",
      "Number of training records: 29643\n",
      "Converting relation ids to labels: 100%|█| 29643/29643 [01:28<00:00, 336.45it/s]\n",
      "Training samples are saved to data/wikidata-simplequestions/dataset/train.jsonl\n",
      "Processing valid data...\n",
      "Negative sampling: 100%|████████████████████| 2210/2210 [00:26<00:00, 84.60it/s]\n",
      "Number of training records: 4397\n",
      "Converting relation ids to labels: 100%|███| 4397/4397 [00:13<00:00, 326.37it/s]\n",
      "Training samples are saved to data/wikidata-simplequestions/dataset/valid.jsonl\n",
      "Processing test data...\n",
      "Negative sampling: 100%|████████████████████| 4296/4296 [00:48<00:00, 89.45it/s]\n",
      "Number of training records: 8539\n",
      "Converting relation ids to labels: 100%|███| 8539/8539 [00:25<00:00, 331.68it/s]\n",
      "Training samples are saved to data/wikidata-simplequestions/dataset/test.jsonl\n"
     ]
    }
   ],
   "source": [
    "for split in splits:\n",
    "    print(f'Processing {split} data...')\n",
    "    scored_path = os.path.join(intermediate_dir, f'scores_{split}.jsonl')\n",
    "    dataset_path = os.path.join(dataset_dir, f'{split}.jsonl')\n",
    "    !srtk preprocess -i $scored_path \\\n",
    "        -o $dataset_path \\\n",
    "        -e http://localhost:1234/api/endpoint/sparql \\\n",
    "        -kg wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"query\":\"what is the language in which mera shikar was filmed in [SEP] \",\"positive\":\"original language of film or TV show\",\"negatives\":[\"composer\",\"instance of\",\"cast member\",\"director\",\"instance of\",\"composer\",\"country of origin\",\"country of origin\",\"cast member\",\"cast member\",\"color\",\"cast member\",\"instance of\",\"country of origin\",\"cast member\"]}\n",
      "{\"query\":\"what is the language in which mera shikar was filmed in [SEP] original language of film or TV show\",\"positive\":\"END OF HOP\",\"negatives\":[\"has grammatical case\",\"has grammatical case\",\"described by source\",\"different from\",\"indigenous to\",\"on focus list of Wikimedia project\",\"has grammatical gender\",\"instance of\",\"different from\",\"writing system\",\"subclass of\",\"instance of\",\"country\",\"subclass of\",\"writing system\"]}\n",
      "{\"query\":\"what is the position that  mike twellman plays [SEP] \",\"positive\":\"position played on team / speciality\",\"negatives\":[\"country of citizenship\",\"sex or gender\",\"occupation\",\"educated at\",\"family name\",\"sport\",\"member of sports team\",\"educated at\",\"place of birth\",\"country of citizenship\",\"country for sport\",\"sex or gender\",\"family name\",\"place of birth\",\"country of citizenship\"]}\n",
      "{\"query\":\"what is the position that  mike twellman plays [SEP] position played on team / speciality\",\"positive\":\"END OF HOP\",\"negatives\":[\"topic's main category\",\"instance of\",\"sport\",\"topic's main category\",\"part of\",\"sport\",\"opposite of\",\"part of\",\"opposite of\",\"sport\",\"part of\",\"sport\",\"topic's main category\",\"subclass of\",\"opposite of\"]}\n",
      "{\"query\":\"what is ellen swallow richards's nationality? [SEP] \",\"positive\":\"country of citizenship\",\"negatives\":[\"residence\",\"employer\",\"maintained by WikiProject\",\"employer\",\"spouse\",\"residence\",\"writing language\",\"archives at\",\"writing language\",\"educated at\",\"place of death\",\"on focus list of Wikimedia project\",\"field of work\",\"described by source\",\"influenced by\"]}\n"
     ]
    }
   ],
   "source": [
    "train_dataset_path = os.path.join(dataset_dir, 'train.jsonl')\n",
    "!head -n 5 $train_dataset_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train a scorer model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a retrieval model is as easy as running a single command! Just pass in the pretrained language model name and the training dataset path. We'll handle the rest.\n",
    "\n",
    "We recommend you to register an accound at [WanDB](https://docs.wandb.ai/quickstart), and log in in the command line. That's it, then you'll then be able to track your training progress in an online dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = os.path.join(dataset_dir, 'train.jsonl')\n",
    "validation_dataset_path = os.path.join(dataset_dir, 'valid.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Found cached dataset json (/home/wiss/liao/.cache/huggingface/datasets/json/default-e38cde1f0c05f759/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-e38cde1f0c05f759/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e9d6249f4bca8eea.arrow\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-e38cde1f0c05f759/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-975a9100c709b3dd.arrow\n",
      "Found cached dataset json (/home/wiss/liao/.cache/huggingface/datasets/json/default-c5042479f0114657/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-c5042479f0114657/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-65b67668a1d714c5.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanchun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1martifacts/wandb/run-20230428_233419-vtg7f3h7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m04282334\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/vtg7f3h7\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | RobertaModel | 124 M \n",
      "---------------------------------------\n",
      "124 M     Trainable params\n",
      "0         Non-trainable params\n",
      "124 M     Total params\n",
      "498.583   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|████████████████████| 309/309 [04:51<00:00,  1.06it/s, v_num=f3h7]\n",
      "Validation:   0%|                                        | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|████████████████████| 309/309 [04:50<00:00,  1.06it/s, v_num=f3h7]\u001b[A\n",
      "Validation:   0%|                                        | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|████████████████████| 309/309 [04:50<00:00,  1.07it/s, v_num=f3h7]\u001b[A\n",
      "Validation:   0%|                                        | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|████████████████████| 309/309 [04:50<00:00,  1.06it/s, v_num=f3h7]\u001b[A\n",
      "Validation:   0%|                                        | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|████████████████████| 309/309 [04:50<00:00,  1.06it/s, v_num=f3h7]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|████████████████████| 309/309 [05:05<00:00,  1.01it/s, v_num=f3h7]\u001b[A`Trainer.fit` stopped: `max_epochs=5` reached.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch ▁▁▁▁▁▁▁▃▃▃▃▃▃▃▅▅▅▅▅▅▅▆▆▆▆▆▆▆███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss █▃▃▃▂▂▂▁▁▁▁▁▁▁▂▁▁▃▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss █▄▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.00471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/global_step 1544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.02491\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m04282334\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/vtg7f3h7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1martifacts/wandb/run-20230428_233419-vtg7f3h7/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 srtk train -t $train_dataset_path \\\n",
    "    -v $validation_dataset_path \\\n",
    "    --model-name-or-path roberta-base \\\n",
    "    --output-dir $output_model_dir \\\n",
    "    --accelerator gpu \\\n",
    "    --learning-rate 1e-5 \\\n",
    "    --batch-size 96 \\\n",
    "    --max-epochs 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By clicking the wandb link shown in the output, you can easily monitor the training progress in a dashboard. \n",
    "\n",
    "![wandb training progress](https://i.imgur.com/4zKmzBT.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Evaluate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may evaluate the trained retriever (or to say, the scorer) by passing `--evaluate` argument to the `retrieve` subcommand. The model is evaluated by the number of samples that ground truth entities are retieved by the number of total test samples.\n",
    "\n",
    "The file for evaluation should at least contain the following fields:\n",
    "```json\n",
    "{\n",
    "    \"question\": \"What is the birth place of Barack Obama?\",\n",
    "    \"question_entities\": [\"Q76\"],\n",
    "    \"answer_entities\": [\"Q23513\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scored_path = os.path.join(intermediate_dir, 'scores_test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subgraphs: 100%|█████████████████| 4296/4296 [02:47<00:00, 25.61it/s]\n",
      "Retrieved subgraphs saved to to artifacts/subgraphs/wd_simple.jsonl\n",
      "Answer recall: 0.9748603351955307 (4188 / 4296)\n"
     ]
    }
   ],
   "source": [
    "!srtk retrieve -i $test_scored_path \\\n",
    "    -o $retrieved_subgraph_path \\\n",
    "    -e http://localhost:1234/api/endpoint/sparql \\\n",
    "    -kg wikidata \\\n",
    "    --scorer-model-path $output_model_dir \\\n",
    "    --beam-width 20 \\\n",
    "    --max-depth 1 \\\n",
    "    --evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. (Optional) Share your model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to [share a model](https://huggingface.co/docs/transformers/model_sharing) from HuggingFace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
