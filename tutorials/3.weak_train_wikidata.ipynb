{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Retriever in a Distant Supervision setting\n",
    "\n",
    "**Why Distant Supervision**\n",
    "\n",
    "A retriever has to know the correct path from the question entities to answer entities. A traditional approach for collecting training data thus is to manually annotate a reasoning path in the knowledge graph. Such annotation is much more complex than traditional annotation tasks such as classification. It is also very expensive and time-consuming.\n",
    "\n",
    "An alternative approach to generate training data is **distant supervision**. In this setting, given question entities and answer entities, we use the shortest path between them as the reasoning path. This is a reasonable assumption because the shortest path is often the most relevant path. This approach is much cheaper than manual annotation and can be easily scaled to large datasets.\n",
    "\n",
    "**Data**\n",
    "\n",
    "- Knowledge graph: [Wikidata](https://www.wikidata.org)\n",
    "- Dataset: [Mintaka](https://huggingface.co/datasets/AmazonScience/mintaka)\n",
    "- Note: In Mintaka, question entities and answer entities are annotated, but the path is not known."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install & import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install srtk datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import srsly\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Wikidata SPARQL endpoint. Please refer to [Setup Wikidata]() for setup instructions. We assume that:\n",
    "\n",
    "- the SPARQL endpoint service deployed at: `http://localhost:1234/api/endpoint/sparql`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths and other constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('data/mintaka/')\n",
    "data_root.mkdir(parents=True, exist_ok=True)\n",
    "converted_dataset_path = data_root / 'dataset.jsonl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prepare Training Data\n",
    "\n",
    "In this step, we load and convert Mintaka dataset to the required format. Each sample of the training data should be prepared in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"sample-id\",\n",
    "  \"question\": \"Which universities did Barack Obama graduate from?\",\n",
    "  \"question_entities\": [\n",
    "    \"Q76\"\n",
    "  ],\n",
    "  \"answer_entities\": [\n",
    "    \"Q49122\",\n",
    "    \"Q1346110\",\n",
    "    \"Q4569677\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Mintaka Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: mintaka/en\n",
      "Found cached dataset mintaka (/home/wiss/liao/.cache/huggingface/datasets/AmazonScience___mintaka/en/1.0.0/bb35d95f07aed78fa590601245009c5f585efe909dbd4a8f2a4025ccf65bb11d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'lang', 'question', 'answerText', 'category', 'complexityType', 'questionEntity', 'answerEntity'],\n",
      "    num_rows: 14000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'a9011ddf',\n",
       " 'lang': 'en',\n",
       " 'question': 'What is the seventh tallest mountain in North America?',\n",
       " 'answerText': 'Mount Lucania',\n",
       " 'category': 'geography',\n",
       " 'complexityType': 'ordinal',\n",
       " 'questionEntity': [{'name': 'Q49',\n",
       "   'entityType': 'entity',\n",
       "   'label': 'North America',\n",
       "   'mention': 'North America',\n",
       "   'span': [40, 53]},\n",
       "  {'name': '7',\n",
       "   'entityType': 'ordinal',\n",
       "   'label': '',\n",
       "   'mention': 'seventh',\n",
       "   'span': [12, 19]}],\n",
       " 'answerEntity': [{'name': 'Q1153188', 'label': 'Mount Lucania'}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from huggingface datasets\n",
    "mintaka = load_dataset('AmazonScience/mintaka', split='train')\n",
    "# Show the metadata of the dataset\n",
    "print(mintaka)\n",
    "# Examine a sample\n",
    "mintaka[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert and Filter Data\n",
    "\n",
    "The Mintaka dataset contains questions that may not have annotations for question entities and answer entities. Therefore, we exclude any samples where either the question entities or answer entities are not annotated, or are not in the form of Wikidata entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14000/14000 [00:01<00:00, 8354.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9880 samples, skipped 4120 samples, total 14000 samples\n",
      "Output saved to data/mintaka/dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skipped = 0\n",
    "processed_samples = []\n",
    "for sample in tqdm(mintaka, desc='Preparing samples'):\n",
    "    question_entities = [e['name'] for e in sample['questionEntity'] if e['entityType']=='entity']\n",
    "    answer_entities = [e['name'] for e in sample['answerEntity']]\n",
    "    if len(question_entities) == 0 or len(answer_entities) == 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    processed_sample = {\n",
    "        'id': str(sample['id']),\n",
    "        'question': sample['question'],\n",
    "        'question_entities': question_entities,\n",
    "        'answer_entities': answer_entities,\n",
    "    }\n",
    "    processed_samples.append(processed_sample)\n",
    "\n",
    "srsly.write_jsonl(converted_dataset_path, processed_samples)\n",
    "print(f'Processed {len(processed_samples)} samples, skipped {skipped} samples, total {len(mintaka)} samples')\n",
    "print(f'Output saved to {converted_dataset_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Preprocess the Training Data\n",
    "\n",
    "\n",
    "To streamline the preprocessing of training data, `srtk preprocess` can be used. This command performs the following operations:\n",
    "\n",
    "1. It searches for the shortest paths between `question_entities` and `answer_entities` in the knowledge graph. These paths consist of a chain of relations.\n",
    "2. The paths are then scored based on the Jaccard score between the answer entities and the entities derived from the question entities along the path.\n",
    "3. Negative sampling of relations is then performed.\n",
    "4. Finally, training samples are generated. Each sample consists of:\n",
    "    - a question plus previous relations\n",
    "    - the next positive relation\n",
    "    - k negative relations (where k defaults to 15).\n",
    "\n",
    "As a result, three files are generated in the output directory:\n",
    "\n",
    "- `paths.jsonl`: contains the shortest paths between question entities and answer entities.\n",
    "- `scores.jsonl`: contains the scores of the paths.\n",
    "- `train.jsonl`: contains the training samples, in which negative samples are also included.\n",
    "\n",
    "\n",
    "For more information on the preprocessing options, use the command `srtk preprocess --help`. Additional details about the preprocessing pipeline can be found in the [Preprocessing documentation]()."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess and create the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching paths: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9880/9880 [06:30<00:00, 25.30it/s]\n",
      "Processed 5711 samples; skipped 4169 samples without any paths between question entities and answer entities; total 9880 samples\n",
      "Retrieved paths saved to data/mintaka/paths.jsonl\n",
      "Scoring paths: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5711/5711 [01:16<00:00, 74.87it/s]\n",
      "Scored paths saved to data/mintaka/scores.jsonl\n",
      "Negative sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5711/5711 [03:24<00:00, 27.89it/s]\n",
      "Number of training records: 25107\n",
      "Converting relation ids to labels: 100%|â–ˆ| 25107/25107 [00:39<00:00, 628.79it/s]\n",
      "Training samples are saved to data/mintaka/train.jsonl\n"
     ]
    }
   ],
   "source": [
    "!srtk preprocess --input $converted_dataset_path \\\n",
    "    --output-dir $data_root \\\n",
    "    --sparql-endpoint http://localhost:1234/api/endpoint/sparql \\\n",
    "    --knowledge-graph wikidata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Inspect the training data that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;39m{\n",
      "  \u001b[0m\u001b[34;1m\"query\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Which actor starred in Vanilla Sky and was married to Katie Holmes? [SEP] \"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"positive\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"spouse\"\u001b[0m\u001b[1;39m,\n",
      "  \u001b[0m\u001b[34;1m\"negatives\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
      "    \u001b[0;32m\"cast member\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"composer\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"child\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"eye color\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"filming location\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"given name\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"instance of\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"different from\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"child\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"given name\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"original language of film or TV show\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"described by source\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"distributed by\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"distributed by\"\u001b[0m\u001b[1;39m,\n",
      "    \u001b[0;32m\"screenwriter\"\u001b[0m\u001b[1;39m\n",
      "  \u001b[1;39m]\u001b[0m\u001b[1;39m\n",
      "\u001b[1;39m}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/mintaka/train.jsonl | jq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train the retriever\n",
    "\n",
    "With `srtk train`, you can train a retriever (i.e. a relation scorer) with a single command. The improtant arguments inlcude:\n",
    "\n",
    "- `input`: This specifies the path of the training data generated in the previous step.\n",
    "- `model_name_or_path`: This specifies the pretrained model to be used, and can be any HuggingFace model identifier or a local path to a model.\n",
    "- `accelerator`: This specifies the accelerator to be used, and can be `cpu`, `gpu`, or `tpu`.\n",
    "- `output_dir`: This specifies the directory where the trained model will be saved. The model is saved in HuggingFace model format, which can be uploaded to the HuggingFace hub and shared with the community.\n",
    "\n",
    "Additionally, common training arguments like `max_epochs` and `batch_size` can also be passed to the command.\n",
    "\n",
    "Internally, a [PyTorch Lightning](https://www.pytorchlightning.ai/index.html) trainer is used to train the model, which is also wrapped with PyTorch Lightning Module. [Wandb](https://wandb.ai/) logger is used to log the training progress and metrics.\n",
    "\n",
    "For more information on the training options, use the command `srtk train --help`. Additional details about the training pipeline can be found in the [Training documentation]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at smallbenchnlp/roberta-small were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at smallbenchnlp/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset json (/home/wiss/liao/.cache/huggingface/datasets/json/default-665ea510edb1d62a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-665ea510edb1d62a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c03367c34174058.arrow\n",
      "Loading cached processed dataset at /home/wiss/liao/.cache/huggingface/datasets/json/default-665ea510edb1d62a/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-67e702a35d0e9e94.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuanchun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1martifacts/wandb/run-20230423_143924-284pkty9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m23-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/284pkty9\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Some weights of the model checkpoint at smallbenchnlp/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at smallbenchnlp/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at smallbenchnlp/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at smallbenchnlp/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at smallbenchnlp/roberta-small were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at smallbenchnlp/roberta-small and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type         | Params\n",
      "---------------------------------------\n",
      "0 | model | RobertaModel | 22.4 M\n",
      "---------------------------------------\n",
      "22.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.777    Total estimated model params size (MB)\n",
      "/home/stud/liao/miniconda3/envs/dragon/lib/python3.8/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.24s/it, v_num=]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.76it/s]\u001b[A/home/stud/liao/miniconda3/envs/dragon/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "  warning_cache.warn(\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.03s/it, v_num=]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.03s/it, v_num=]`Trainer.fit` stopped: `max_steps=1` reached.\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.03s/it, v_num=]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33m23-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/yuanchun/retrieval/runs/284pkty9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1martifacts/wandb/run-20230423_143924-284pkty9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!srtk train --input data/mintaka/train.jsonl \\\n",
    "    --output-dir artifacts/mintaka \\\n",
    "    --model-name-or-path smallbenchnlp/roberta-small \\\n",
    "    --accelerator gpu \\\n",
    "    --fast-dev-run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dragon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
